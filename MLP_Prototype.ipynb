{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback, LearningRateScheduler, LambdaCallback\n",
    "from keras.callbacks import TerminateOnNaN, EarlyStopping, ReduceLROnPlateau, CSVLogger, ModelCheckpoint\n",
    "\n",
    "from keras.metrics import categorical_accuracy, binary_accuracy\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import StratifiedKFold, KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables to be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "taskname = \"verytoxic\"             ## Specify task to be predicted\n",
    "tasktype = \"classification\"  ## Specify either classification or regression\n",
    "datarep = \"image\"          ## Specify data representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Specify dataset name\n",
    "jobname = \"tox_niehs\"\n",
    "\n",
    "# Specify location of data\n",
    "homedir = os.path.expanduser(\"~/\")\n",
    "if datarep == \"image\":\n",
    "    archdir = homedir+\"AIChem/archive/\"\n",
    "    K.set_image_dim_ordering('tf')\n",
    "    pixel = 80\n",
    "    num_channel = 4\n",
    "    channel = \"engA\"\n",
    "elif datarep == \"tabular\":\n",
    "    archdir = homedir+\"AIChem/chemnet/chemnet/data/\"\n",
    "elif datarep == \"text\":\n",
    "    archdir = homedir+\"AIChem/chemnet/chemnet/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chem_scripts import cs_load_csv, cs_load_smiles, cs_load_image, cs_create_dict, cs_prep_data_X, cs_prep_data_y, cs_data_balance\n",
    "from chem_scripts import cs_compute_results, cs_keras_to_seaborn, cs_make_plots\n",
    "from chem_scripts import cs_setup_mlp, cs_setup_rnn, cs_setup_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if datarep == \"tabular\":\n",
    "\n",
    "    # Load training + validation data\n",
    "    filename=archdir+jobname+\"_tv_\"+taskname+\"_rdkit.csv\"\n",
    "    X, y = cs_load_csv(filename)\n",
    "\n",
    "    # Load test data\n",
    "    filename=archdir+jobname+\"_int_\"+taskname+\"_rdkit.csv\"\n",
    "    X_test, y_test = cs_load_csv(filename)\n",
    "    if tasktype == \"classification\":\n",
    "        y_test, _ = cs_prep_data_y(y_test, tasktype=tasktype)\n",
    "        \n",
    "elif datarep == \"text\":\n",
    "    \n",
    "    # Load training + validation data\n",
    "    filename=archdir+jobname+\"_tv_\"+taskname+\"_smiles.csv\"\n",
    "    X, y = cs_load_smiles(filename)\n",
    "    \n",
    "    # Load test data\n",
    "    filename=archdir+jobname+\"_int_\"+taskname+\"_smiles.csv\"\n",
    "    X_test, y_test = cs_load_smiles(filename)\n",
    "    if tasktype == \"classification\":\n",
    "        y_test, _ = cs_prep_data_y(y_test, tasktype=tasktype)\n",
    "    \n",
    "    # Create dictionary\n",
    "    characters, char_table, char_lookup = cs_create_dict(X, X_test)\n",
    "    \n",
    "    # Map chars to integers\n",
    "    X = cs_prep_data_X(X, datarep=datarep, char_table=char_table)\n",
    "    X_test = cs_prep_data_X(X_test, datarep=datarep, char_table=char_table)\n",
    "    \n",
    "elif datarep == \"image\":\n",
    "\n",
    "    # Load training + validation data\n",
    "    filename=archdir+jobname+\"_tv_\"+taskname\n",
    "    X, y = cs_load_image(filename, channel=channel)\n",
    "\n",
    "    # Load test data\n",
    "    filename=archdir+jobname+\"_int_\"+taskname \n",
    "    X_test, y_test = cs_load_image(filename, channel=channel)\n",
    "    if tasktype == \"classification\":\n",
    "        y_test, _ = cs_prep_data_y(y_test, tasktype=tasktype)\n",
    "\n",
    "    # Reshape X to be [samples][channels][width][height]\n",
    "    X = X.reshape(X.shape[0], pixel, pixel, num_channel).astype(\"float32\")\n",
    "    X_test = X_test.reshape(X_test.shape[0], pixel, pixel, num_channel).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def f_nn():\n",
    "\n",
    "    # Define counter for hyperparam iterations\n",
    "    global run_counter\n",
    "    run_counter += 1\n",
    "        \n",
    "    print('*** TRIAL: '+str(run_counter))\n",
    "    print('*** PARAMETERS TESTING: '+str(params))\n",
    "    \n",
    "    # Intialize results file\n",
    "    # Setup cross-validation\n",
    "    if tasktype == \"classification\":\n",
    "        cv_results = pd.DataFrame(columns=['Train Loss', 'Validation Loss', 'Test Loss', 'Train AUC', 'Validation AUC', 'Test AUC'])\n",
    "        stratk = StratifiedKFold(n_splits=5, random_state=7)\n",
    "        splits = stratk.split(X, y)\n",
    "    elif tasktype == \"regression\":\n",
    "        cv_results = pd.DataFrame(columns=['Train Loss', 'Validation Loss', 'Test Loss', 'Train RMSE', 'Validation RMSE', 'Test RMSE'])\n",
    "        stratk = KFold(n_splits=5, random_state=7)\n",
    "        splits = stratk.split(X, y)\n",
    "\n",
    "    # Do cross-validation\n",
    "    for i, (train_index, valid_index) in enumerate(splits):\n",
    "\n",
    "        if prototype == True:\n",
    "            if i > 0:\n",
    "                break\n",
    "\n",
    "        print(\"\\nOn CV iteration: \"+str(i))\n",
    "        # Do standard k-fold splitting\n",
    "        X_train, y_train = X[train_index], y[train_index]\n",
    "        X_valid, y_valid = X[valid_index], y[valid_index]\n",
    "        print(\"BEFORE Sampling: \"+str(i)+\" Train: \"+str(X_train.shape)+\" Valid: \"+str(X_valid.shape))\n",
    "        print(\"BEFORE Sampling: \"+str(i)+\" Train: \"+str(y_train.shape)+\" Valid: \"+str(y_valid.shape))\n",
    "\n",
    "        if tasktype == \"classification\":\n",
    "        \n",
    "            # Do class-balancing\n",
    "            balanced_indices = cs_data_balance(y_train)\n",
    "            X_train = X_train[balanced_indices]\n",
    "            y_train = y_train[balanced_indices]\n",
    "            \n",
    "            balanced_indices = cs_data_balance(y_valid)\n",
    "            X_valid = X_valid[balanced_indices]\n",
    "            y_valid = y_valid[balanced_indices]\n",
    "            \n",
    "            # One-hot encoding\n",
    "            y_train, y_class = cs_prep_data_y(y_train, tasktype=tasktype)   #ONLY DO THIS AFTER SPLITTING\n",
    "            y_valid, y_class = cs_prep_data_y(y_valid, tasktype=tasktype)\n",
    "            print(\"AFTER Sampling: \"+str(i)+\" Train: \"+str(X_train.shape)+\" Valid: \"+str(X_valid.shape))\n",
    "            print(\"AFTER Sampling: \"+str(i)+\" Train: \"+str(y_train.shape)+\" Valid: \"+str(y_valid.shape))\n",
    "            \n",
    "        elif tasktype == \"regression\":\n",
    "            \n",
    "            y_class = 1\n",
    "\n",
    "        # Setup network\n",
    "        if datarep == \"tabular\":\n",
    "            model, submodel = cs_setup_mlp(params, inshape=X_train.shape[1], classes=y_class)\n",
    "        elif datarep == \"text\":\n",
    "            model, submodel = cs_setup_rnn(params, inshape=X_train.shape[1], classes=y_class, char=characters)\n",
    "        elif datarep == \"image\":\n",
    "            model, submodel = cs_setup_cnn(params, inshape=(pixel, pixel, num_channel), classes=y_class)\n",
    "        \n",
    "        if i == 0:\n",
    "            # Print architecture\n",
    "            print(model.summary())\n",
    "            # Save model\n",
    "            model_json = submodel.to_json()\n",
    "            filemodel=jobname+\"_\"+taskname+\"_architecture_\"+str(run_counter)+\".json\"\n",
    "            with open(filemodel, \"w\") as json_file:\n",
    "                json_file.write(model_json)\n",
    "\n",
    "        # Setup callbacks\n",
    "        filecp = jobname+\"_\"+taskname+\"_bestweights_trial_\"+str(run_counter)+\"_\"+str(i)+\".hdf5\"\n",
    "        filecsv = jobname+\"_\"+taskname+\"_loss_curve_\"+str(run_counter)+\"_\"+str(i)+\".csv\"\n",
    "        callbacks = [TerminateOnNaN(),\n",
    "                     LambdaCallback(on_epoch_end=lambda epoch,logs: sys.stdout.flush()),\n",
    "                     EarlyStopping(monitor='val_loss', patience=25, verbose=1, mode='auto'),\n",
    "                     ModelCheckpoint(filecp, monitor=\"val_loss\", verbose=1, save_best_only=True, mode=\"auto\"),\n",
    "                     CSVLogger(filecsv)]\n",
    "        \n",
    "        # Train model\n",
    "        if datarep == \"image\":\n",
    "            datagen = ImageDataGenerator(rotation_range=180, fill_mode='constant', cval=0.)\n",
    "            hist = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),\n",
    "                                       epochs=nb_epoch, steps_per_epoch=X_train.shape[0]/batch_size,\n",
    "                                       verbose=verbose,\n",
    "                                       validation_data=(X_valid, y_valid),\n",
    "                                       callbacks=callbacks)\n",
    "        else:\n",
    "            hist = model.fit(x=X_train, y=y_train,\n",
    "                             batch_size=batch_size,\n",
    "                             epochs=nb_epoch,\n",
    "                             verbose=verbose,\n",
    "                             validation_data=(X_valid, y_valid),\n",
    "                             callbacks=callbacks)\n",
    "\n",
    "        # Visualize loss curve\n",
    "        hist_df = cs_keras_to_seaborn(hist)\n",
    "        cs_make_plots(hist_df)\n",
    "        \n",
    "        # Reload best model & compute results\n",
    "        model.load_weights(filecp)\n",
    "        cs_compute_results(model, classes=y_class, df_out=cv_results,\n",
    "                           train_data=(X_train,y_train),\n",
    "                           valid_data=(X_valid,y_valid),\n",
    "                           test_data=(X_test,y_test))\n",
    "    \n",
    "    # Calculate results for entire CV\n",
    "    final_mean = cv_results.mean(axis=0)\n",
    "    final_std = cv_results.std(axis=0)\n",
    "    cv_results.to_csv('results.csv', index=False)\n",
    "    \n",
    "    # Print final results\n",
    "    print('*** TRIAL RESULTS: '+str(run_counter))\n",
    "    print('*** PARAMETERS TESTED: '+str(params))\n",
    "    \n",
    "    if tasktype == \"regression\":\n",
    "        print(('train_loss: %.3f +/- %.3f, train_rmse: %.3f +/- %.3f, val_loss: %.3f +/- %.3f, val_rmse: %.3f +/- %.3f, test_loss: %.3f +/- %.3f, test_rmse: %.3f +/- %.3f')\n",
    "              %(final_mean[0], final_std[0], final_mean[3], final_std[3],\n",
    "                final_mean[1], final_std[1], final_mean[4], final_std[4],\n",
    "                final_mean[2], final_std[2], final_mean[5], final_std[5]))\n",
    "    elif tasktype == \"classification\":\n",
    "        print(('train_loss: %.3f +/- %.3f, train_auc: %.3f +/- %.3f, val_loss: %.3f +/- %.3f, val_auc: %.3f +/- %.3f, test_loss: %.3f +/- %.3f, test_auc: %.3f +/- %.3f')\n",
    "              %(final_mean[0], final_std[0], final_mean[3], final_std[3],\n",
    "                final_mean[1], final_std[1], final_mean[4], final_std[4],\n",
    "                final_mean[2], final_std[2], final_mean[5], final_std[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Network hyperparameters\n",
    "\n",
    "# Hyperparams:\n",
    "# Dropout: 0 to 0.5 (float)\n",
    "# num_layer: 2 to 6 (int)\n",
    "# relu_type: relu, elu, leakyrelu, prelu\n",
    "# layerN_units: 16, 32, 64, 128, 256 (int)\n",
    "# reg_flag: l1, l2, l1_l2, none\n",
    "# reg_val: 1 to 6 (float)\n",
    "if datarep == \"tabular\":\n",
    "    params = {\"dropval\":0.5, \"num_layer\":2, \"relu_type\":\"prelu\",\n",
    "              \"layer1_units\":128, \"layer2_units\":128, \"layer3_units\":128,\n",
    "              \"layer4_units\":128, \"layer5_units\":128, \"layer6_units\":128,\n",
    "              \"reg_type\": \"l2\", \"reg_val\": 2.5 }\n",
    "    \n",
    "# Hyperparams:\n",
    "# Dropout: 0 to 0.5 (float)\n",
    "# em_dim: 1 to 10 (int)\n",
    "# num_layer: 1 to 3 (int)\n",
    "# relu_type: relu, elu, leakyrelu, prelu\n",
    "# conv units: 16, 32, 64, 128, 256 (int)\n",
    "# layerN_units: 16, 32, 64, 128, 256 (int)\n",
    "elif datarep == \"text\":\n",
    "    params = {\"em_dim\":5, \"conv_units\":6, \"dropval\":0.5, \"num_layer\":2,\n",
    "              \"celltype\":\"GRU\", \"relu_type\":\"prelu\",\n",
    "              \"layer1_units\":12, \"layer2_units\":12, \"layer3_units\":12,\n",
    "              \"reg_type\": \"l2\", \"reg_val\": 2}\n",
    "\n",
    "# Hyperparams:\n",
    "# Dropout: 0 to 0.5 (float)\n",
    "# num_blockN: 1 to 5 (int)\n",
    "# convN units: 16, 32, 64, 128, 256 (int)\n",
    "elif datarep == \"image\":\n",
    "    params = {\"conv1_units\":32, \"conv2_units\":32, \"conv3_units\":32,\n",
    "              \"conv4_units\":32, \"conv5_units\":32, \"conv6_units\":32,\n",
    "              \"num_block1\":3, \"num_block2\":3, \"num_block3\":3, \"dropval\":0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run settings\n",
    "\n",
    "run_counter = 0\n",
    "batch_size = 128\n",
    "nb_epoch = 5\n",
    "verbose = 1\n",
    "prototype = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f_nn()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
